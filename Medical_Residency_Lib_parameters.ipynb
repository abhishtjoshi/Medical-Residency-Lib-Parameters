{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Libraries used in the Experimentation"
      ],
      "metadata": {
        "id": "P2tcPAekWCT6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. PyTorch\n",
        "\n",
        "2. Pandas\n",
        "3. Scikit-Learn\n",
        "4. NLTK\n",
        "5. Numpy\n",
        "6. Seaborn\n",
        "7. NLTK\n",
        "8. BERTopic\n",
        "9. SHAP\n",
        "10. Spacy\n",
        "11. Unidecode"
      ],
      "metadata": {
        "id": "Z0_uiB-lWINf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Text Cleaning**"
      ],
      "metadata": {
        "id": "32bee_XIobi-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sw = stopwords.words('english')\n",
        "\n",
        "def clean_text(text):\n",
        "    # remove\n",
        "    text = re.sub('\\[.*?\\]', '', text)\n",
        "    # remove links\n",
        "    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
        "    # remove tags\n",
        "    text = re.sub('<.*?>+', '', text)\n",
        "    # remove punctuation\n",
        "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
        "    # remove breaklines\n",
        "    text = re.sub('\\n', '', text)\n",
        "    # remove numbers\n",
        "    text = re.sub('\\w*\\d\\w*', '', text)\n",
        "\n",
        "    # remove accent\n",
        "    text = unidecode.unidecode(text)\n",
        "\n",
        "    # transform text into token\n",
        "    text_token = nltk.word_tokenize(text)\n",
        "\n",
        "    # remove stopwords\n",
        "    words = [w for w in text_token if w not in sw]\n",
        "\n",
        "    return ' '.join(words)"
      ],
      "metadata": {
        "id": "QATuNJzXofo2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Lemmatization**"
      ],
      "metadata": {
        "id": "BQLVQ3iVpfiE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Text Lemmatization\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def lemmatize_sentence(text):\n",
        "    # transform text into token\n",
        "    text_token = nltk.word_tokenize(text)\n",
        "    lemmatized_sentence = []\n",
        "    for word in text_token:\n",
        "        lemmatized_sentence.append(word)\n",
        "    return \" \".join(lemmatized_sentence)"
      ],
      "metadata": {
        "id": "IHpbijqNpf8H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**BERTopic**"
      ],
      "metadata": {
        "id": "FJMTNgHhvo_t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('wordnet')\n",
        "wn = nltk.WordNetLemmatizer()\n",
        "# Topic model\n",
        "from bertopic import BERTopic\n",
        "# Dimension reduction\n",
        "from umap import UMAP"
      ],
      "metadata": {
        "id": "F-YOBP1DvpHc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initiate UMAP\n",
        "umap_model = UMAP(n_neighbors=6,\n",
        "                  n_components=3,\n",
        "                  min_dist=0.2,\n",
        "                  metric='cosine',\n",
        "                  random_state=42)\n",
        "# Initiate BERTopic\n",
        "topic_model = BERTopic(umap_model=umap_model, language=\"multilingual\", calculate_probabilities=True)\n",
        "# Run BERTopic model\n",
        "topics, probabilities = topic_model.fit_transform(data)\n",
        "# Visualize top topic keywords/?\n",
        "topic_model.visualize_barchart(top_n_topics=8)"
      ],
      "metadata": {
        "id": "j29uiDekvr7-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize term rank decrease\n",
        "topic_model.visualize_term_rank()"
      ],
      "metadata": {
        "id": "14AkmOppvykd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize connections between topics using hierachical clustering\n",
        "topic_model.visualize_hierarchy(top_n_topics=12)"
      ],
      "metadata": {
        "id": "cEMsSMPbwdzf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SHAP**"
      ],
      "metadata": {
        "id": "rJluFSDyP-j8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "shap.initjs()"
      ],
      "metadata": {
        "id": "fJhqn34rQAXQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "explainer = shap.KernelExplainer(model.predict_proba, X_train)\n",
        "shap_values = explainer.shap_values(X_test)"
      ],
      "metadata": {
        "id": "lLEfDplSQHrN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "explainer = shap.KernelExplainer(model.predict_proba, X_train)\n",
        "shap_values = explainer.shap_values(X_test)"
      ],
      "metadata": {
        "id": "wCW21sOwQLte"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "explainer = shap.sample(X_train,50)\n",
        "shap_values = explainer.shap_values(X_test)"
      ],
      "metadata": {
        "id": "kJVOtLQKQPnW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Principal Component Analysis (PCA)**"
      ],
      "metadata": {
        "id": "16Hd8dNVueb5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#PCA to reduce input features\n",
        "transformer = PCA(n_components = 0.99)\n",
        "transformer.fit(X_train)\n",
        "pca_cols = []\n",
        "for i in range(transformer.n_components_):\n",
        "    pca_cols.append(\"pc\"+str(i))\n",
        "X_train = pd.DataFrame(transformer.transform(X_train),\n",
        "                 columns=pca_cols)\n",
        "X_test = pd.DataFrame(transformer.transform(X_test),\n",
        "                 columns=pca_cols)"
      ],
      "metadata": {
        "id": "kE4J8rMoue4l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SMOTE Implementation**"
      ],
      "metadata": {
        "id": "1sPtwdhqh5kY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "oversample = SMOTE(k_neighbors=2)\n",
        "X_train, y_train = oversample.fit_resample(X_train, y_train)"
      ],
      "metadata": {
        "id": "Rw-_L-GXWHwV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Mutual Information**"
      ],
      "metadata": {
        "id": "EeMIj_nijiWh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#feature selection using Mutual Info Gain\n",
        "mutual_info = mutual_info_classif(X_train, y_train, random_state= 42)\n",
        "mutual_info = dict(zip(X_train.columns, mutual_info))\n",
        "# Plotthing the ordered mutual_info values per feature\n",
        "bar_ind = pd.Series(mutual_info)\n",
        "bar_ind = bar_ind.sort_values(ascending=False)\n",
        "info_gain_list = []\n",
        "for x in bar_ind.keys():\n",
        "    if bar_ind[x]>0.1:\n",
        "        info_gain_list.append(x)"
      ],
      "metadata": {
        "id": "PPoj1bY-jmUd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#let's plot the ordered mutual_info values per feature\n",
        "bar_ind = pd.Series(mutual_info)\n",
        "bar_ind.sort_values(ascending=False).plot.bar(figsize=(20, 8))"
      ],
      "metadata": {
        "id": "gc6j44aojvoE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Model Parameters**"
      ],
      "metadata": {
        "id": "1tPKMyUMieB3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**LSTM**"
      ],
      "metadata": {
        "id": "HQhmOBD3zP04"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_words = 500\n",
        "max_len = 50\n",
        "\n",
        "def tokenize_pad_sequences(text):\n",
        "    '''\n",
        "    This function tokenize the input text into sequnences of intergers and then\n",
        "    pad each sequence to the same length\n",
        "    '''\n",
        "    # Text tokenization\n",
        "    tokenizer = Tokenizer(num_words=max_words, lower=True, split=' ')\n",
        "    tokenizer.fit_on_texts(text)\n",
        "    # Transforms text to a sequence of integers\n",
        "    X = tokenizer.texts_to_sequences(text)\n",
        "    # Pad sequences to the same length\n",
        "    X = pad_sequences(X, padding='post', maxlen=max_len)\n",
        "    # return sequences\n",
        "    return X, tokenizer"
      ],
      "metadata": {
        "id": "FkRruE5izQLb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X, tokenizer = tokenize_pad_sequences(data)\n",
        "X_trn, X_tst, y_trn, y_tst = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "X_trn, X_vld, y_trn, y_vld = train_test_split(X_trn, y_trn, test_size=0.3, random_state=42, stratify=y_trn)\n"
      ],
      "metadata": {
        "id": "_3NfHitdzSCf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 5000\n",
        "embedding_size = 32\n",
        "epochs=8\n",
        "max_len = 512\n",
        "\n",
        "model= Sequential()\n",
        "model.add(Embedding(vocab_size, embedding_size, input_length=max_len))\n",
        "model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
        "model.add(MaxPooling1D(pool_size=2))\n",
        "model.add(Bidirectional(LSTM(32)))\n",
        "model.add(Dropout(0.1))\n",
        "model.add(Dense(4, activation='sigmoid'))\n",
        "model.build((None, 100))\n",
        "\n",
        "plot_model(model, show_shapes = True)"
      ],
      "metadata": {
        "id": "oUAHsDGHz5Et"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy',tf.keras.metrics.Precision(), tf.keras.metrics.Recall()])\n",
        "print(model.summary())"
      ],
      "metadata": {
        "id": "yy66dWO50Cqm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "es = EarlyStopping(monitor = 'val_loss', patience=5)\n",
        "batch_size = 64\n",
        "\n",
        "history = model.fit(X_trn, y_trn,\n",
        "                    validation_data=(X_vld, y_vld),\n",
        "                    batch_size=batch_size, epochs=6, verbose=1,\n",
        "                    callbacks = [es])"
      ],
      "metadata": {
        "id": "XqzDt5l30-mn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**XLNET**"
      ],
      "metadata": {
        "id": "MbgZNOHDLIUW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create sentence and label lists\n",
        "sentences = data.text_lm.values\n",
        "\n",
        "sentences = [sentence + \" [SEP] [CLS]\" for sentence in sentences]\n",
        "labels = data.Labels.values"
      ],
      "metadata": {
        "id": "TMrkxGlGLK_0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = XLNetTokenizer.from_pretrained('xlnet-base-cased', do_lower_case=True)\n",
        "\n",
        "tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n",
        "print (\"Tokenize the first sentence:\")\n",
        "print (tokenized_texts[0])\n",
        "\n",
        "# Create attention masks\n",
        "attention_masks = []\n",
        "\n",
        "# Create a mask of 1s for each token followed by 0s for padding\n",
        "for seq in input_ids:\n",
        "  seq_mask = [float(i>0) for i in seq]\n",
        "  attention_masks.append(seq_mask)\n",
        "\n",
        "# Use train_test_split to split our data into train and validation sets for training\n",
        "\n",
        "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels,\n",
        "                                                            random_state=2018, test_size=0.1)\n",
        "train_masks, validation_masks, _, _ = train_test_split(attention_masks, input_ids,\n",
        "                                             random_state=2018, test_size=0.1)"
      ],
      "metadata": {
        "id": "wPleB34yM6ib"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the maximum sequence length. The longest sequence in our training set is 47, but we'll leave room on the end anyway.\n",
        "MAX_LEN = 128\n",
        "# Use the XLNet tokenizer to convert the tokens to their index numbers in the XLNet vocabulary\n",
        "input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
        "# Pad our input tokens\n",
        "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")"
      ],
      "metadata": {
        "id": "uCw92KrCNAy1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert all of our data into torch tensors, the required datatype for our model\n",
        "\n",
        "train_inputs = torch.tensor(train_inputs)\n",
        "validation_inputs = torch.tensor(validation_inputs)\n",
        "train_labels = torch.tensor(train_labels)\n",
        "validation_labels = torch.tensor(validation_labels)\n",
        "train_masks = torch.tensor(train_masks)\n",
        "validation_masks = torch.tensor(validation_masks)"
      ],
      "metadata": {
        "id": "oWGPxySBNOa9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Select a batch size for training. For fine-tuning with XLNet, the authors recommend a batch size of 32, 48, or 128. We will use 32 here to avoid memory issues.\n",
        "batch_size = 32\n",
        "\n",
        "# Create an iterator of our data with torch DataLoader. This helps save on memory during training because, unlike a for loop,\n",
        "# with an iterator the entire dataset does not need to be loaded into memory\n",
        "\n",
        "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
        "validation_sampler = SequentialSampler(validation_data)\n",
        "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "y0Ta6R5WNRpj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "from transformers import XLNetForSequenceClassification\n",
        "\n",
        "model = XLNetForSequenceClassification.from_pretrained('xlnet-base-cased', num_labels=2)\n",
        "\n",
        "model.cuda()\n",
        "\n",
        "param_optimizer = list(model.named_parameters())\n",
        "no_decay = ['bias','gamma','beta']\n",
        "optimizer_grouped_parameters = [\n",
        "    {'params':[p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
        "      'weight_decay_rate':0.01},\n",
        "    {'params':[p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
        "      'weight_decay_rate':0.0}\n",
        "]"
      ],
      "metadata": {
        "id": "NXgBmxXlNU_r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to calculate the accuracy of our predictions vs labels\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
      ],
      "metadata": {
        "id": "XhrCPAZhOvkL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Store our loss and accuracy for plotting\n",
        "train_loss_set = []\n",
        "\n",
        "# Number of training epochs (authors recommend between 2 and 4)\n",
        "epochs = 4\n",
        "\n",
        "# trange is a tqdm wrapper around the normal python range\n",
        "for _ in trange(epochs, desc=\"Epoch\"):\n",
        "\n",
        "\n",
        "  # Training\n",
        "\n",
        "  # Set our model to training mode (as opposed to evaluation mode)\n",
        "  model.train()\n",
        "\n",
        "  # Tracking variables\n",
        "  tr_loss = 0\n",
        "  nb_tr_examples, nb_tr_steps = 0, 0\n",
        "\n",
        "  # Train the data for one epoch\n",
        "  for step, batch in enumerate(train_dataloader):\n",
        "    # Add batch to GPU\n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "    # Unpack the inputs from our dataloader\n",
        "    b_input_ids, b_input_mask, b_labels = batch\n",
        "    # Clear out the gradients (by default they accumulate)\n",
        "    optimizer.zero_grad()\n",
        "    # Forward pass\n",
        "    outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
        "    loss = outputs[0]\n",
        "    logits = outputs[1]\n",
        "    train_loss_set.append(loss.item())\n",
        "    # Backward pass\n",
        "    loss.backward()\n",
        "    # Update parameters and take a step using the computed gradient\n",
        "    optimizer.step()\n",
        "\n",
        "\n",
        "    # Update tracking variables\n",
        "    tr_loss += loss.item()\n",
        "    nb_tr_examples += b_input_ids.size(0)\n",
        "    nb_tr_steps += 1\n",
        "\n",
        "  print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))\n"
      ],
      "metadata": {
        "id": "_5M-0cVgO-JI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CatBoost**"
      ],
      "metadata": {
        "id": "4UfqrD8gk4Xn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from catboost import Pool\n",
        "train_pool = Pool(X_train,y_train)\n",
        "validate_pool = Pool(X_test,y_test)\n",
        "params = {\n",
        "    'leaf_estimation_method': 'Gradient',\n",
        "    'learning_rate': 1,\n",
        "    'max_depth': 3,\n",
        "    'bootstrap_type': 'Bernoulli',\n",
        "    'objective': 'MultiClass',\n",
        "\n",
        "\n",
        "    'subsample': 0.8,\n",
        "    'random_state': 42,\n",
        "    'verbose': 0,\n",
        "    \"eval_metric\" : 'TotalF1',\n",
        "    \"early_stopping_rounds\" : 100\n",
        "    }\n",
        "\n",
        "\n",
        "model = CatBoostClassifier(**params)\n",
        "model.fit(train_pool, eval_set=validate_pool)"
      ],
      "metadata": {
        "id": "e4uQRq5sWF7B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CatBoost with GridSearchCV\n",
        "\n",
        "parameters = {'depth'         : [4,5,6,7],\n",
        "                 'learning_rate' : [0.01,0.04],\n",
        "                  'iterations'    : [10, 20,30],\n",
        "              'verbose': [False]\n",
        "                 }\n",
        "CBC = CatBoostClassifier()\n",
        "Grid_CBC = GridSearchCV(estimator=CBC, param_grid = parameters)\n",
        "Grid_CBC.fit(X_train, y_train)\n",
        "cbc_best_params = Grid_CBC.best_params_\n",
        "cbc_best_params"
      ],
      "metadata": {
        "id": "0sBiNmLKwzDI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SVM**"
      ],
      "metadata": {
        "id": "TsY6QszKk8Bk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SVM = SVC(probability=True)\n",
        "param={'kernel':('linear', 'poly','rbf'),\n",
        "      'C':np.arange(1,5),\n",
        "      'gamma': [0.1, 1],\n",
        "      'degree':[2,4]}\n",
        "\n",
        "SVM_grid = GridSearchCV(estimator = SVM,\n",
        "                       param_grid = param)"
      ],
      "metadata": {
        "id": "YbB0rkgMw_Lo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Random Forest with GridsearchCV**"
      ],
      "metadata": {
        "id": "GyfBXxUGvKwX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of trees in random forest\n",
        "n_estimators = [20,60,100,120]\n",
        "\n",
        "# Number of features to consider at every split\n",
        "max_features = [0.2,0.6,1.0]\n",
        "\n",
        "# Maximum number of levels in tree\n",
        "max_depth = [2,8,None]\n",
        "\n",
        "# Number of samples\n",
        "max_samples = [0.5,0.75,1.0]\n",
        "param_grid = {'n_estimators': n_estimators,\n",
        "               'max_features': max_features,\n",
        "               'max_depth': max_depth,\n",
        "              'max_samples':max_samples\n",
        "             }"
      ],
      "metadata": {
        "id": "-SwmnhHwvK4_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "RF = RandomForestClassifier()\n",
        "rf_grid = GridSearchCV(estimator = RF,\n",
        "                       param_grid = param_grid,\n",
        "                       n_jobs = -1)"
      ],
      "metadata": {
        "id": "UG45hIgovO7N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rf_grid.fit(X_train,y_train)\n",
        "rf_best_params = rf_grid.best_params_\n",
        "rf_best_params"
      ],
      "metadata": {
        "id": "Rj7VVeTiqcs1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Multi Layer Perceptron (MLP) with GridsearchCV**"
      ],
      "metadata": {
        "id": "MRYcQw7bwSzy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "params_mlp = {\n",
        "    'hidden_layer_sizes': [(10,20,30)],\n",
        "    'activation': ['tanh','relu','sigmoid'],\n",
        "    'solver': ['adam'],\n",
        "    'alpha': [0.0001],\n",
        "    'learning_rate': ['constant', 'adaptive'],\n",
        "    'random_state': [42]\n",
        "}\n",
        "mlp = GridSearchCV(MLPClassifier(max_iter=1000), params_mlp, n_jobs=-1).fit(X_train, y_train)\n",
        "mlp_best_params = mlp.best_params_\n",
        "mlp_best_params"
      ],
      "metadata": {
        "id": "LG9NjE_pwS75"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Pipeline for Code**"
      ],
      "metadata": {
        "id": "zyIjx8CsxFA2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "classifiers = [RandomForestClassifier(**rf_best_params),\n",
        "              SVC(**svm_best_params),\n",
        "               CatBoostClassifier(**cbc_best_params),\n",
        "              MLPClassifier(**mlp_best_params)]\n",
        "scoring = {'accuracy' : make_scorer(accuracy_score),\n",
        "           'precision' : make_scorer(precision_score,pos_label='positive',average='macro'),\n",
        "           'recall' : make_scorer(recall_score,pos_label='positive',average='macro'),\n",
        "           'f1_score' : make_scorer(f1_score,pos_label='positive',average='macro')}\n",
        "all_scores = []\n",
        "f1_scores = []\n",
        "accuracy_scores = []\n",
        "\n",
        "precision_scores = []\n",
        "recall_scores = []\n",
        "for clf in classifiers:\n",
        "    clf_pipe = Pipeline([('scale',MinMaxScaler()),('smt', SMOTE(random_state=43)), ('pca', PCA(n_components=0.99)),\n",
        "                              ('cls', clf)] )\n",
        "    scores = cross_validate(clf_pipe, X, Y, cv=StratifiedKFold(n_splits=10),scoring=scoring)\n",
        "    mean_scores = {}\n",
        "    all_scores.append(scores)\n",
        "    for x in scores.keys():\n",
        "        mean_scores[x] = scores[x].mean()\n",
        "    f1_scores.append(mean_scores['test_f1_score'])\n",
        "    accuracy_scores.append(mean_scores['test_accuracy'])\n",
        "    precision_scores.append(mean_scores['test_precision'])\n",
        "    recall_scores.append(mean_scores['test_recall'])\n",
        "    print(type(clf).__name__,mean_scores)"
      ],
      "metadata": {
        "id": "dWm1-6EZxFJb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}